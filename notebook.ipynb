{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live-stock detection (DeepForest)\n",
    "## Context\n",
    "### Purpose\n",
    "Implement and fine-tune a pre-trained Deep Learning model to detect live-stock in airborne imagery.\n",
    "\n",
    "\n",
    "### Modelling Approach\n",
    "The [live-stock detection model](https://deepforest.readthedocs.io/en/latest/prebuilt.html#livestock-detectors) from the latest version (v1.4.0) of the [DeepForest](https://deepforest.readthedocs.io/en/latest/) Deep Learning model is used to predict bounding boxes corresponding to cattle from airborn RGB images.\n",
    "\n",
    "The prebuilt model was trained on a [limited dataset](https://new.wildlabs.net/discussion/global-model-livestock-detection-airborne-imagery-data-applications-and-needs). According to the package's documentation, \"the prebuilt models will always be improved by adding data from the target area\". As such, this notebook will explore the improvement in the model's performance in live-stock detection from fine-tuning on local data.\n",
    "\n",
    "### Description\n",
    "This notebook will explore the capabilities of the DeepForest package. In particular, it will demonstrate how to:\n",
    "\n",
    "- Detect live-stock in airborne imagery using the prebuilt live-stock detection model.\n",
    "- Fine-tune the model using a novel publicly-available dataset.\n",
    "- Evaluate the the model's performance before and after fine-tuning.\n",
    "\n",
    "### Highlights\n",
    "*Provide 3-5 bullet points that convey the use case’s core procedures. Each bullet point must have a maximum of 85 characters, including spaces.*\n",
    "* Highlight 1\n",
    "* Highlight 2\n",
    "\n",
    "### Contributions\n",
    "#### Notebook\n",
    "* Cameron Appel (author), Queen Mary University of London, @camappel\n",
    "\n",
    "#### Modelling codebase\n",
    "* Ben Weinstein (maintainer & developer), University of Florida, @bw4sz\n",
    "* Henry Senyondo (support maintainer), University of Florida, @henrykironde\n",
    "* Ethan White (PI and author), University of Florida, @weecology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "List libraries according to their role e.g. system/files manipulation i.e. os (first), data handling i.e. numpy, xarray (second), visualisation i.e. holoviews (third), etc. The cell below contains two libraries, `os` and `warning` which are common among the Python Jupyter notebooks. Don't remove them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torchvision\n",
    "!pip -q install git+https://github.com/Weecology/DeepForest.git\n",
    "!pip -q install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import intake\n",
    "import xmltodict\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepforest import main\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "\n",
    "from shapely.geometry import box\n",
    "from skimage.exposure import equalize_hist\n",
    "\n",
    "import pooch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set project structure\n",
    "*The cell below creates a separate folder to save the notebook outputs. This facilitates the reader to inspect inputs/outputs stored within a defined destination folder. Don't remove the lines below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "notebook_folder = './notebook'\n",
    "if not os.path.exists(notebook_folder):\n",
    "    os.makedirs(notebook_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = os.path.join(notebook_folder, 'test_data')\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch RGB images from Zenodo\n",
    "\n",
    "Fetch sample images from Harvard's publicly accessible [ODjAR Dataverse](https://dataverse.harvard.edu/dataverse/ODjAR). \n",
    "\n",
    "Specifically, G.J. Franke; Sander Mucher, 2021, \"Annotated cows in aerial images for use in deep learning models\", which includes \"a large dataset containing aerial images from fields in Juchowo, Poland and Wageningen, the Netherlands, with annotated cows present in the images using Pascal VOC XML Annotation Format.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_files = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13851270/test_data.zip\",\n",
    "    known_hash=\"6a0a5b48fc9326e97c3cd8bdcabc2bcd131f3755f6ceabbf6976aefbfc87fb00\",\n",
    "    processor=pooch.Unzip(extract_dir=extract_dir),\n",
    "    path=f\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV (annotations), assuming it's also part of the unzipped files\n",
    "test_path = [file for file in unzipped_files if file.endswith('test.csv')][0]\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /Users/cam/miniconda3/envs/deepforest/lib/python3.10/site-packages/deepforest/data/deepforest_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /Users/cam/miniconda3/envs/deepforest/lib/python3.10/site-packages/deepforest/data/deepforest_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1d851d5eb54a7cbeb5ada0e2984539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/129M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "deepforest(\n",
       "  (model): RetinaNet(\n",
       "    (backbone): BackboneWithFPN(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fpn): FeaturePyramidNetwork(\n",
       "        (inner_blocks): ModuleList(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (layer_blocks): ModuleList(\n",
       "          (0-2): 3 x Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (extra_blocks): LastLevelP6P7(\n",
       "          (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RetinaNetHead(\n",
       "      (classification_head): RetinaNetClassificationHead(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cls_logits): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (regression_head): RetinaNetRegressionHead(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "    )\n",
       "  )\n",
       "  (iou_metric): IntersectionOverUnion()\n",
       "  (mAP_metric): MeanAveragePrecision()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = main.deepforest()\n",
    "model.load_model(model_name=\"weecology/deepforest-livestock\", revision=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9a9b6142e341d6a5d4438476449038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/miniconda3/envs/deepforest/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation complete. Results saved to ./notebook/baseline_pred_result\n"
     ]
    }
   ],
   "source": [
    "model.label_dict = {'cow': 0}  # Assign a unique integer ID to the 'cow' label\n",
    "model.config['gpus'] = '-1'  # Use GPU (set to '0' for the first GPU or '-1' for all GPUs)\n",
    "\n",
    "# Set the directory to save the results of the pretrained model\n",
    "baseline_save_dir = os.path.join(notebook_folder, 'baseline_pred_result')\n",
    "os.makedirs(baseline_save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate the pretrained model on the test set (using test_file)\n",
    "baseline_results = model.evaluate(test_path, os.path.dirname(test_path), iou_threshold=0.4, savedir=baseline_save_dir)\n",
    "\n",
    "print(\"Baseline evaluation complete. Results saved to\", baseline_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box Recall: 0.4405\n",
      "Box Precision: 0.5826\n",
      "Mean IoU: 0.3135\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline performance\")\n",
    "# Print box recall and precision with clean formatting\n",
    "print(f\"Box Recall: {baseline_results['box_recall']:.4f}\")\n",
    "print(f\"Box Precision: {baseline_results['box_precision']:.4f}\")\n",
    "\n",
    "# Compute and print the mean IoU, rounded to 4 decimal places\n",
    "mean_iou = np.mean(baseline_results['results']['IoU'])\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdfb7424fd842e095cd3b3de86fcb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finetuned_checkpoint.ckpt:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file: /Users/cam/miniconda3/envs/deepforest/lib/python3.10/site-packages/deepforest/data/deepforest_config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download the finetuned model checkpoint from Hugging Face\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"camappel/deepforest-livestock\",  # Replace with your Hugging Face repo ID\n",
    "    filename=\"finetuned_checkpoint.ckpt\"     # The .ckpt file you uploaded\n",
    ")\n",
    "\n",
    "# Load the model checkpoint correctly using the class, not an instance\n",
    "model = main.deepforest.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "print(\"Finetuned model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate finetuned performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0df71661b642ca9be046729909783d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cam/miniconda3/envs/deepforest/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.16 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned evaluation complete. Results saved to ./notebook/finetuned_pred_result\n"
     ]
    }
   ],
   "source": [
    "model.label_dict = {'cow': 0}  # Assign a unique integer ID to the 'cow' label\n",
    "model.config['gpus'] = '-1'  # Use GPU (set to '0' for the first GPU or '-1' for all GPUs)\n",
    "\n",
    "\n",
    "# Set the directory to save the results of the pretrained model\n",
    "finetuned_save_dir = os.path.join(notebook_folder, 'finetuned_pred_result')\n",
    "os.makedirs(finetuned_save_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate the pretrained model on the test set (using test_file)\n",
    "finetuned_results = model.evaluate(test_path, os.path.dirname(test_path), iou_threshold=0.4, savedir=finetuned_save_dir)\n",
    "\n",
    "print(\"Finetuned evaluation complete. Results saved to\", finetuned_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned performance\n",
      "Box Recall: 0.9535\n",
      "Box Precision: 0.8587\n",
      "Mean IoU: 0.6571\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetuned performance\")\n",
    "# Print box recall and precision with clean formatting\n",
    "print(f\"Box Recall: {finetuned_results['box_recall']:.4f}\")\n",
    "print(f\"Box Precision: {finetuned_results['box_precision']:.4f}\")\n",
    "\n",
    "# Compute and print the mean IoU, rounded to 4 decimal places\n",
    "mean_iou = np.mean(finetuned_results['results']['IoU'])\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the paths of all downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Iterate over all files in the registry dictionary to download them\n",
    "for zip_file in data.registry.keys():\n",
    "    file_path = data.fetch(zip_file)\n",
    "    downloaded_files.append(file_path)\n",
    "    print(f\"Downloaded {zip_file} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = []\n",
    "for zip_file in data.registry.keys():\n",
    "    file_path = os.path.join(data.abspath, zip_file)  # Get the absolute path for each downloaded file\n",
    "    downloaded_files.append(file_path)\n",
    "\n",
    "print(notebook_folder)\n",
    "print(downloaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../../../../../../../OneDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../../../../../../../Documents/notebook_combined')\n",
    "\n",
    "# Combine all parts into a single archive using py7zr\n",
    "combined_file_path = os.path.join('../../../../../../../Documents/notebook_combined', \"Annotated_cows_GenTORE.7z\")\n",
    "\n",
    "with open(combined_file_path, 'wb') as combined_file:\n",
    "    for part in sorted(downloaded_files):\n",
    "        with open(part, 'rb') as part_file:\n",
    "            combined_file.write(part_file.read())\n",
    "\n",
    "print(combined_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the combined 7z archive\n",
    "with py7zr.SevenZipFile(combined_file_path, mode='r') as archive:\n",
    "    archive.extractall(path='../../../../../../../Documents/notebook_combined')\n",
    "\n",
    "print(f\"All files have been extracted to ./notebook_combined'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and combine without saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the notebook folder path\n",
    "notebook_folder = './notebook'\n",
    "\n",
    "# Create the notebook folder if it does not exist\n",
    "if not os.path.exists(notebook_folder):\n",
    "    os.makedirs(notebook_folder)\n",
    "\n",
    "# Create a Pooch object for handling data\n",
    "data = pooch.create(\n",
    "    base_url=\"doi:10.7910/DVN/N7GJYU\", \n",
    "    path=pooch.os_cache(\"myproject\")\n",
    ")\n",
    "\n",
    "# Load the registry from the DOI\n",
    "data.load_registry_from_doi()\n",
    "\n",
    "# List to hold the paths of all downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# First loop to download all files\n",
    "for zip_file in data.registry.keys():\n",
    "    file_path = data.fetch(zip_file)  # Fetch and download each file\n",
    "    downloaded_files.append(file_path)  # Collect all downloaded files\n",
    "    print(f\"Downloaded {zip_file} to {file_path}\")\n",
    "\n",
    "print(\"All parts have been downloaded successfully.\")\n",
    "\n",
    "# Direct extraction from the first part using py7zr\n",
    "# Ensure all parts (.001, .002, etc.) are present in the same directory\n",
    "with py7zr.SevenZipFile(downloaded_files[0], mode='r') as archive:\n",
    "    archive.extractall(path=notebook_folder)\n",
    "\n",
    "print(f\"All files have been extracted to '{notebook_folder}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch RGB images from Zenodo\n",
    "\n",
    "Fetch sample images from the publicly accessible [NEON](https://zenodo.org/records/3459803) training set to evaluate DeepForest's detection and classification performance on different types of landscapes\n",
    "\n",
    "### Small distinct tree-crowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find hash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"2018_SJER_3_258000_4106000_image.tif\"\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=f\"doi:10.5281/zenodo.3459803/{fname}\",\n",
    "    known_hash=\"md5:d70ecbee40abe043946e8e492c514a63\",\n",
    "    path=notebook_folder,\n",
    "    fname=fname\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set catalogue location\n",
    "catalog_file = os.path.join(notebook_folder, 'catalog.yaml')\n",
    "\n",
    "with open(catalog_file, 'w') as f:\n",
    "    f.write(f'''\n",
    "sources:\n",
    "  NEONTREE_rgb:\n",
    "    driver: xarray_image\n",
    "    description: 'NeonTreeEvaluation RGB images (collection)'\n",
    "    args:\n",
    "      urlpath: \"{{{{ CATALOG_DIR }}}}/{fname}\"\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an intake catalog for the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tc = intake.open_catalog(catalog_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use intake to load the sample image through dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_rgb = cat_tc[\"NEONTREE_rgb\"].to_dask()\n",
    "tc_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to load xml and extract bounding boxes\n",
    "\n",
    "# function to create ordered dictionary of .xml annotation files\n",
    "def loadxml(imagename):\n",
    "    imagename = imagename.replace('.tif','')\n",
    "    fullurl = \"https://raw.githubusercontent.com/weecology/NeonTreeEvaluation/master/annotations/\" + imagename + \".xml\"\n",
    "    file = urllib.request.urlopen(fullurl)\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    data = xmltodict.parse(data)\n",
    "    return data\n",
    "\n",
    "# function to extract bounding boxes\n",
    "def extractbb(i):\n",
    "    bb = [f['bndbox'] for f in xml['annotation']['object']]\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = loadxml(fname)\n",
    "bball = extractbb(xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise image and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot images\n",
    "def cv2_imshow(a, **kwargs):\n",
    "    a = a.clip(0, 255).astype('uint8')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return plt.imshow(a, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tc_rgb\n",
    "\n",
    "# plot predicted bbox\n",
    "image2 = image.values.copy()\n",
    "target_bbox = bball\n",
    "print(type(target_bbox))\n",
    "print(target_bbox[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in target_bbox:\n",
    "    cv2.rectangle(image2, (int(row[\"xmin\"]), int(row[\"ymin\"])), (int(row[\"xmax\"]), int(row[\"ymax\"])), (0,255,255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "plot_reference = plt.figure(figsize=(15,15))\n",
    "cv2_imshow(np.flip(image2,2))\n",
    "plt.title('Reference labels',fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large dense canopies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"2018_NIWO_2_450000_4426000_image_crop.tif\"\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=f\"doi:10.5281/zenodo.3459803/{fname}\",\n",
    "    known_hash=\"md5:c8f700eca920c6f0b93d16e6e26cc5a7\",\n",
    "    path=notebook_folder,\n",
    "    fname=fname\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set catalogue location\n",
    "catalog_file = os.path.join(notebook_folder, 'catalog.yaml')\n",
    "\n",
    "with open(catalog_file, 'w') as f:\n",
    "    f.write(f'''\n",
    "sources:\n",
    "  NEONTREE_rgb:\n",
    "    driver: xarray_image\n",
    "    description: 'NeonTreeEvaluation RGB images (collection)'\n",
    "    args:\n",
    "      urlpath: \"{{{{ CATALOG_DIR }}}}/{fname}\"\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an intake catalog for the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tc = intake.open_catalog(catalog_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use intake to load the sample image through dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_rgb = cat_tc[\"NEONTREE_rgb\"].to_dask()\n",
    "tc_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = loadxml(fname)\n",
    "bball = extractbb(xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise image and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot images\n",
    "def cv2_imshow(a, **kwargs):\n",
    "    a = a.clip(0, 255).astype('uint8')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return plt.imshow(a, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tc_rgb\n",
    "\n",
    "# plot predicted bbox\n",
    "image2 = image.values.copy()\n",
    "target_bbox = bball\n",
    "print(type(target_bbox))\n",
    "print(target_bbox[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in target_bbox:\n",
    "    cv2.rectangle(image2, (int(row[\"xmin\"]), int(row[\"ymin\"])), (int(row[\"xmax\"]), int(row[\"ymax\"])), (0,255,255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "plot_reference = plt.figure(figsize=(15,15))\n",
    "cv2_imshow(np.flip(image2,2))\n",
    "plt.title('Reference labels',fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DeepForest pretrained model\n",
    "\n",
    "Now we're going to load and use a pretrained model from the deepforest package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepforest import main\n",
    "\n",
    "# load deep forest model\n",
    "model = main.deepforest()\n",
    "model.use_release()\n",
    "model.current_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes = model.predict_image(image=image.values)\n",
    "print(pred_boxes.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = image.values.copy() \n",
    "\n",
    "for index, row in pred_boxes.iterrows():\n",
    "    cv2.rectangle(image3, (int(row[\"xmin\"]), int(row[\"ymin\"])), (int(row[\"xmax\"]), int(row[\"ymax\"])), (0,255,255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "plot_fullimage = plt.figure(figsize=(15,15))\n",
    "cv2_imshow(np.flip(image3,2))\n",
    "plt.title('Full-image predictions',fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison full image prediction and reference labels\n",
    "Let's compare the labels and predictions over the tested image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_referandfullimage = plt.figure(figsize=(15,15))\n",
    "ax1 = plt.subplot(1, 2, 1), cv2_imshow(np.flip(image2,2))\n",
    "ax1[0].set_title('Reference labels',fontsize='xx-large')\n",
    "ax2 = plt.subplot(1, 2, 2), cv2_imshow(np.flip(image3,2))\n",
    "ax2[0].set_title('Full-image predictions', fontsize='xx-large')\n",
    "plt.show() # To show figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile-based prediction\n",
    "To optimise the predictions, the DeepForest can be run tile-wise.\n",
    "\n",
    "The following cells show how to define the optimal window i.e. tile size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from deepforest import preprocess\n",
    "\n",
    "#Create windows of 400px\n",
    "windows = preprocess.compute_windows(image.values, patch_size=400,patch_overlap=0)\n",
    "print(f'We have {len(windows)} windows in the image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through a few sample windows, crop and predict\n",
    "plot_tilewindows, axes, = plt.subplots(nrows=2,ncols=2, figsize=(15,15))\n",
    "axes = axes.flatten()\n",
    "for index2 in range(4):\n",
    "    crop = image.values[windows[index2].indices()]\n",
    "    #predict in bgr channel order, color predictions in red.\n",
    "    boxes = model.predict_image(image=np.flip(crop[...,::-1],2), return_plot = True)\n",
    "\n",
    "    #but plot in rgb channel order\n",
    "    axes[index2].imshow(boxes[...,::-1])\n",
    "    axes[index2].set_title(f'Prediction in Window {index2 + 1} out of {len(windows)}', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a suitable tile size is defined, we can run in a batch using the predict_tile function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = model.predict_tile(image=image.values,return_plot=False,patch_overlap=0,iou_threshold=0.05,patch_size=400)\n",
    "\n",
    "# plot predicted bbox\n",
    "image_tile = image.values.copy()\n",
    "\n",
    "for index, row in tile.iterrows():\n",
    "    cv2.rectangle(image_tile, (int(row[\"xmin\"]), int(row[\"ymin\"])), (int(row[\"xmax\"]), int(row[\"ymax\"])), (0, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "\n",
    "plot_tilewise = plt.figure(figsize=(15,15))\n",
    "ax1 = plt.subplot(1, 2, 1), cv2_imshow(np.flip(image2,2))\n",
    "ax1[0].set_title('Reference labels',fontsize='xx-large')\n",
    "ax2 = plt.subplot(1, 2, 2), cv2_imshow(np.flip(image_tile,2))\n",
    "ax2[0].set_title('Tile-wise predictions', fontsize='xx-large')\n",
    "plt.show() # To show figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plots\n",
    "\n",
    "The plot below summarises above static plots by interactively comparing bounding boxes and scores of full-image and tile-wise predictions. To zoom-in the reference NEON RGB image with its original resolution change rasterize=True to rasterize=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to convert bbox in dictionary to geopandas\n",
    "def bbox_to_geopandas(bbox_df):\n",
    "    geometry = [box(x1, y1, x2, y2) for x1,y1,x2,y2 in zip(bbox_df.xmin, bbox_df.ymin, bbox_df.xmax, bbox_df.ymax)]\n",
    "    poly_geo = GeoDataFrame(bbox_df, geometry=geometry)\n",
    "    return poly_geo\n",
    "\n",
    "## prepare reference and prediction bbox\n",
    "### convert data types for reference bbox dictionary\n",
    "reference = pd.DataFrame.from_dict(target_bbox)\n",
    "reference[['xmin', 'ymin', 'xmax', 'ymax']] = reference[['xmin', 'ymin', 'xmax', 'ymax']].astype(int)\n",
    "\n",
    "poly_reference = bbox_to_geopandas(reference)\n",
    "poly_prediction_image = bbox_to_geopandas(pred_boxes)\n",
    "poly_prediction_tile = bbox_to_geopandas(tile)\n",
    "\n",
    "## settings for hvplot objects\n",
    "settings_vector = dict(fill_color=None, width=400, height=400, clim=(0,1), fontsize={'title': '110%'})\n",
    "settings_image = dict(x='x', y='y', data_aspect=1, xaxis=False, yaxis=None)\n",
    "\n",
    "## create hvplot objects\n",
    "plot_RGB = tc_rgb.hvplot.rgb(**settings_image, bands='channel', hover=False, rasterize=True)\n",
    "plot_vector_reference = poly_reference.hvplot(hover_cols=False, legend=False).opts(title='Reference labels', alpha=1, **settings_vector)\n",
    "plot_vector_image = poly_prediction_image.hvplot(hover_cols=['score'], legend=False).opts(title='Full-image predictions', alpha=0.5, **settings_vector)\n",
    "plot_vector_tile = poly_prediction_tile.hvplot(hover_cols=['score'], legend=False).opts(title='Tile-wise predictions', alpha=0.5, **settings_vector)\n",
    "\n",
    "plot_comparison = pn.Row(pn.Column(plot_RGB * plot_vector_reference, \n",
    "                         plot_RGB * plot_vector_image),\n",
    "                         pn.Column(pn.Spacer(background='white', width=400, height=400),  \n",
    "                         plot_RGB * plot_vector_tile), scroll=True)\n",
    "\n",
    "plot_comparison.embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from deepforest.model import CropModel\n",
    "\n",
    "model = CropModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "*Provide 3-5 bullet points summarising the main aspects of the dataset and tools covered in the notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentence 1 e.g. `tool-name` to perform...\n",
    "* Sentence 2 e.g. `tool-name` to perform..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "**Dataset**: Type here details of dataset(s) version.\n",
    "\n",
    "**Codebase**: Type here details of codebase version (only for notebooks categorised under modelling/preprocesing/post-processing themes).\n",
    "\n",
    "**License**: The code in this notebook is licensed under the MIT License. The Environmental Data Science book is licensed under the Creative Commons by Attribution 4.0 license. See further details [here](https://github.com/alan-turing-institute/environmental-ds-book/blob/master/LICENSE.md).\n",
    "\n",
    "**Contact**: If you have any suggestion or report an issue with this notebook, feel free to [create an issue](https://github.com/alan-turing-institute/environmental-ds-book/issues/new/choose) or send a direct message to [environmental.ds.book@gmail.com](mailto:environmental.ds.book@gmail.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "print(f'Last tested: {date.today()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
